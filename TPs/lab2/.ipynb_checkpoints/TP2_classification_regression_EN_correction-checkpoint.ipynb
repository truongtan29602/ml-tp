{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Classification and regression\n",
    "\n",
    "We will again use the dataset \"breast cancer\". \n",
    "The task is to classify every instance (i.e. the attributes corresponding to one patient) into one of the two classes: `no-recurrence-events` ou `recurrence-events`.\n",
    "\n",
    "We will experiment with several classification methods and use the python package \"scikit-learn\" (sklearn).\n",
    "\n",
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bc = pd.read_csv(\"breast-cancer/breast-cancer.csv\", skipinitialspace=True)\n",
    "bc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Separate the attributes (that we give as input to the training algorithm) from the labels (that we will use at the output for training and evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = bc.iloc[:,0]\n",
    "data = bc.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical features\n",
    "\n",
    "For the majority of classification algorithms, categorical variables must be encoded in numerical format.\n",
    "\n",
    "(Theoretically, this is not necessary for a decision tree. But for the implementation in scikit-learn, all attributes must be numerical.)\n",
    "\n",
    "Here is a web page with example for pandas:\n",
    "https://machinelearningtutorials.org/pandas-encoding-categorical-features-with-examples/\n",
    "\n",
    "which does not mention the function `factorize()` that can be used for nominal variables (not ordinal).\n",
    "\n",
    "Here is the corresponding scikit-learn documentation:\n",
    "https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features\n",
    "\n",
    "For example, for the \"age\" attribute (ordinal variable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_order = ['20-29', '30-39', '40-49', '50-59', '60-69', '70-79']\n",
    "data['age'] = data['age'].apply(lambda x: age_order.index(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the nominal variable \"menopause\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['menopause'] = data.menopause.factorize()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Replace all columns containing categorical attributes with their numerical version (including the \"labels\").\n",
    "\n",
    "To display all the distinct values of a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tumor-size'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COR\n",
    "ts_order = ['0-4', '5-9', '10-14', '15-19', '20-24', '25-29', '30-34', '35-39', \n",
    "       '40-44', '45-49', '50-54']\n",
    "data['tumor-size'] = data['tumor-size'].apply(lambda x: ts_order.index(x))\n",
    "in_order = ['0-2', '3-5', '6-8', '9-11', '12-14', '15-17', '24-26']\n",
    "data['inv-nodes'] = data['inv-nodes'].apply(lambda x: in_order.index(x))\n",
    "data['node-caps'] = data['node-caps'].factorize()[0]\n",
    "data['breast'] = data['breast'].factorize()[0]\n",
    "data['breat-quad'] = data['breat-quad'].factorize()[0]\n",
    "data['irradiat'] = data['irradiat'].factorize()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COR\n",
    "labels = labels.factorize()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise the data with the fonction \"scatter_matrix\" of pandas (see code below).\n",
    "And do another representation with the fonction \"parallel_coordinates\".\n",
    "\n",
    "### Question 3\n",
    "\n",
    "What conclusions can you draw?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plt.figure(figsize=(15,15))\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "plt.rcParams.update({'font.size': 4})\n",
    "sm = pd.plotting.scatter_matrix(data, c=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_and_labels = pd.concat((data, pd.Series(labels, name=\"class\")), axis=1) # recreate dataframe with labels\n",
    "pd.plotting.parallel_coordinates(data_and_labels, \"class\", color=(\"r\", \"b\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COR\n",
    "\n",
    "# There is no single or pair of attributes that would allow a simple separation of the two classes.\n",
    "# There are attributes that seem to be more discriminant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separation of evaluation data\n",
    "\n",
    "In the following, we will apply Machine Learning methods to our dataset, and we would like to be able to apply it to new data. \n",
    "To evaluate if our classification model gives correct predictions for examples that it has never seen before, we will put aside a part of our data that we will not use to construct the model.\n",
    "\n",
    "### Question 4\n",
    "Separate your DataFrame in two random parts: one for constructing the model (\"train\"), 80%, and the other for evaluating it (\"test\"), 20%.\n",
    "Separate also the corresponding labels. You can use the function `sklearn.model_selection.train_test_split()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COR\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test, train_labels, test_labels = train_test_split(data, labels, stratify=labels, random_state=1, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-NN : k Nearest Neighbours\n",
    "\n",
    "The k-NN method is the most intuitive classification algorithm. \n",
    "To classify a new example, it will determine the $k$ nearest neighbours of that example in the training set and assign the label of the majority of these $k$ neighbours.\n",
    "\n",
    "### Question 5\n",
    "Get familiar with the k-NN algorithm and implement it with the class `KNeighborsClassifier` of \"sklearn\".\n",
    "Train the model on the train set (`fit()`) and apply it (`predict()`) on the train and on the test set.\n",
    "\n",
    "Documentation:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COR\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(train, train_labels)\n",
    "pred = knn.predict(test.loc[:,:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "plt.rcParams['figure.dpi'] = 96\n",
    "plt.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "We will now evalute the performance of our model on the test set.\n",
    "To this end, we will compute the confusion matrix using the function `confusion_matrix(labels, predicted_labels)`.\n",
    "\n",
    "Interpret this matrix and normalise it to percentages/proportions using the parameter: `normalize : {'true', 'pred', 'all'}`.\n",
    "Interpret the output of the different normalisations (the usual choice is \"true\").\n",
    "\n",
    "What is the percentage of correct prediction (accuracy)?\n",
    "You can then use the class `ConfusionMatrixDisplay` to display a graphical representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COR\n",
    "cm = confusion_matrix(test_labels, pred, normalize='true')\n",
    "cm = cm\n",
    "print(cm)\n",
    "\n",
    "# accuracy := sum of the elements on the diagonal (TP + TN)\n",
    "print(f\"Accuracy: {cm[0,0]+cm[1,1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COR\n",
    "cmd = ConfusionMatrixDisplay(cm)\n",
    "cmd.plot()\n",
    "\n",
    "# pour des version plus récénte de sklearn : \n",
    "# ConfusionMatrixDisplay.from_predictions(test_labels, pred)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Question 7\n",
    "Many Machine Learning methods perform better with normalised training data.\n",
    "Use one (or several) of the classes in the package \"sklearn.preprocessing\" (for example \"StandardScaler\") to normalise the training data and compare the obtained model with the one without normalisation.\n",
    "\n",
    "Note: the test set must be normalised with the same parameters as the training set !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COR\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scaler = StandardScaler()\n",
    "train_ = scaler.fit_transform(train)\n",
    "test_ = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COR \n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(train_, train_labels)\n",
    "pred = knn.predict(test_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.score(test_, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "To simplify the processing and to avoid that the test data is used in the training (\"data leackage\") we will combine the preprocessing and training in a so-called \"pipeline\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COR\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "pipe = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    KNeighborsClassifier(n_neighbors=1)\n",
    ")\n",
    "pipe.fit(train, train_labels)\n",
    "pred = pipe.predict(test)\n",
    "accuracy_score(pred, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees\n",
    "\n",
    "We will construct our first \"real\" model: a decision tree using the class 'DecisionTreeClassifier' of sklearn.\n",
    "\n",
    "Decision trees are non-parametric models used in supervised learning for classification and regression.\n",
    "In supervised learning, we observe several discriminant variables and one or several target variables. \n",
    "The goal is to create a model that predicts the target variables using decision rules derived from the data through training.\n",
    "\n",
    "Decision trees describe thus how to partition a population into separate groups that are distinct, but homogenuous  according to a set of discriminant variables and a given objective function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "Create and instance of the class `DecisionTreeClassifier` and construct the model if the method \"fit()\" and the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COR\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier()\n",
    "tree = tree.fit(train, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "Visualise the tree with the method \"plot_tree\" of \"sklearn.tree\" and get familiar with the general model and its construction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COR\n",
    "\n",
    "# import plot_tree\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plot_tree(tree)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "Faire une prédiction avec ce modèle pour tout le jeu de test et évaluer la précision du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COR\n",
    "\n",
    "# prediction\n",
    "pred = tree.predict(test.loc[:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COR\n",
    "\n",
    "tree.score(test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Régression Logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "Train a logistic regression model on our dataset. Use a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COR\n",
    "\n",
    "pipe_logreg = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(random_state=0)\n",
    ")\n",
    "pipe_logreg.fit(train, train_labels)\n",
    "pred = pipe_logreg.predict(test)\n",
    "accuracy_score(pred, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12\n",
    "Train a SVM model on our dataset. Use a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COR\n",
    "\n",
    "pipe_svm = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LinearSVC()\n",
    ")\n",
    "pipe_svm.fit(train, train_labels)\n",
    "pred = pipe_svm.predict(test)\n",
    "accuracy_score(pred, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "The evaluation result depends on the random division into \"train\" and \"test\".\n",
    "To reduice this bias, we can perform a cross-validation dividing the dataset into K parts (folds) of equal size, where K-1 parts are used for training and the remaining part for evaluation.\n",
    "This is repeated K times (k-fold cross-validation) where each of the K parts in turn is used for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "result = cross_validate(pipe, data, labels, cv=10)   # 10-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimising hyper-parameters\n",
    "\n",
    "A hyper-parameter of a model is a parameter that controls, for example, its operation, its architecture or the training algorithm (a sort of \"meta-parameter\"), as opposed to the parameters of the model itself that are optimised during training.\n",
    "\n",
    "### Question 13\n",
    "Vary the parameter $k$ of the k-NN algorithm and evaluate the performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Search\n",
    "The manual search for training hyper-parameters can be very tedious, especially if there are many and if the values are continuous.\n",
    "\n",
    "In sklearn, there are several methods to automate this search.\n",
    "We will experiment with two of them: 'GridSearchCV' and 'RandomizedSearchCV'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'kneighborsclassifier__n_neighbors': range(1,25)}\n",
    "gs = GridSearchCV(pipe, parameters)\n",
    "gs.fit(train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.score(test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "parameters = {'criterion': ['gini', 'entropy', 'log_loss'], \n",
    "              'max_depth': range(1,20),\n",
    "              'min_samples_split': range(2,20)}\n",
    "rs = RandomizedSearchCV(tree, parameters, n_iter=100)\n",
    "rs.fit(train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.score(test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 14\n",
    "Optimise the hyper-parameters of the classifiers: `LogisticRegression` and `LinearSVC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform   # use distributions for continuous-valued hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 15\n",
    "Train a Random Forest and a \"Gradient Boosted Tree\" model on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COR\n",
    "\n",
    "pipe_rf = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    RandomForestClassifier()\n",
    ")\n",
    "pipe_rf.fit(train, train_labels)\n",
    "pred = pipe_rf.predict(test)\n",
    "accuracy_score(pred, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COR\n",
    "\n",
    "pipe_rf = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    #HistGradientBoostingClassifier()  # more efficient for larger datasets\n",
    "    GradientBoostingClassifier()    # for smaller datasets\n",
    ")\n",
    "pipe_rf.fit(train, train_labels)\n",
    "pred = pipe_rf.predict(test)\n",
    "accuracy_score(pred, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "We will now use (again) the CO2 measurement dataset for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlo = pd.read_csv(\"co2_mm_mlo2.csv\", delim_whitespace=True)\n",
    "mlo1 = mlo[mlo[\"monthly_avg\"]>0]\n",
    "plt.plot(mlo1.time, mlo1.monthly_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 16\n",
    "Compute a linear regression model on the first 600 values and apply this model for prediction on the remaining time points.\n",
    "\n",
    "Plot the regression line on top of the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COR\n",
    "\n",
    "# We use the first 600 samples for constructing the model (regression line)\n",
    "train = mlo1.iloc[:600,:]['time'].to_frame()\n",
    "train_labels = mlo1.iloc[:600,:]['monthly_avg']\n",
    "# We use the rest of the data for evaluating the model\n",
    "test = mlo1.iloc[601:,:]['time'].to_frame()\n",
    "test_labels = mlo1.iloc[601:,:]['monthly_avg']\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COR\n",
    "\n",
    "train_pred = regr.predict(train)\n",
    "test_pred = regr.predict(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COR\n",
    "\n",
    "plt.plot(mlo1.time, mlo1.monthly_avg)\n",
    "plt.plot(train, train_pred)\n",
    "plt.plot(test, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 17\n",
    "Output the regression coefficients and evaluate the model using the mean squared error and the R2 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COR\n",
    "\n",
    "print(\"Coefficients: \\n\", regr.coef_, regr.intercept_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(test_labels, test_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(\"Coefficient of determination: %.2f\" % r2_score(test_labels, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
